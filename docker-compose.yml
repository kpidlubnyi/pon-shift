services:
  core:
    build:
      context: .
      dockerfile: ./core/Dockerfile
    container_name: core
    volumes:
      - ./core:/app
      - static_volume:/app/static
      - logs_volume:/app/logs
      - shared_flags:/app/flags
    ports:
      - "8001:8000"
    networks:
      - core-network
    depends_on:
      postgres-db:
        condition: service_healthy
      redis:
        condition: service_healthy
      mongo-db:
        condition: service_healthy
    env_file:
      - .env
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/health/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: unless-stopped

  postgres-db:
    image: postgres:16
    container_name: postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: ${DB_NAME}
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    env_file:
      - .env
    ports:
      - "5433:5432"
    networks:
      - core-network
    command: >
      postgres
      -c shared_buffers=${DB_SHARED_BUFFERS}
      -c maintenance_work_mem=${DB_MAINTENANCE_WORK_MEM}
      -c work_mem=${DB_WORK_MEM}
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h ${DB_HOST} -U ${DB_USER} -d ${DB_NAME}"]
      interval: 10s
      timeout: 5s
      retries: 5

  mongo-db:
    image: mongo:8
    container_name: mongo
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_DB_NAME}
    env_file:
      - .env
    volumes:
      - mongo_data:/data/db
      - ./mongod.conf:/etc/mongod.conf
    command: ['mongod', '--config', "/etc/mongod.conf"]
    networks:
      - core-network
    ports:
      - 27018:27017
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7.2-alpine
    container_name: redis
    volumes:
      - redis_data:/data
    networks:
      - core-network
    ports:
      - 6380:6379
    restart: unless-stopped
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  celery-worker-default:
    build:
      context: .
      dockerfile: ./core/Dockerfile
    container_name: celery-worker-default
    entrypoint: []
    command: celery -A core worker -Q default --loglevel=info --concurrency=4 -n default@%h
    volumes:
      - ./core:/app
      - logs_volume:/app/logs
    depends_on:
      core:
        condition: service_healthy
      postgres-db:
        condition: service_healthy
      mongo-db:
        condition: service_healthy
      redis:
        condition: service_healthy
    env_file:
      - .env
    networks:
      - core-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "celery -A core inspect ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  celery-worker-gtfs:
    build:
      context: .
      dockerfile: ./core/Dockerfile
    container_name: celery-worker-gtfs
    entrypoint: []
    command: celery -A core worker -Q gtfs_updates --loglevel=info --concurrency=1 -n gtfs@%h
    volumes:
      - ./core:/app
      - logs_volume:/app/logs
    depends_on:
      core:
        condition: service_healthy
      postgres-db:
        condition: service_healthy
      mongo-db:
        condition: service_healthy
      redis:
        condition: service_healthy
    env_file:
      - .env
    networks:
      - core-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "celery -A core inspect ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  celery-beat:
    build:
      context: .
      dockerfile: ./core/Dockerfile
    container_name: celery-beat
    entrypoint: []
    command: celery -A core beat --loglevel=info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    volumes:
      - ./core:/app
      - logs_volume:/app/logs
    depends_on:
      celery-worker-default:
        condition: service_healthy
      celery-worker-gtfs:
        condition: service_healthy
      postgres-db:
        condition: service_healthy
      redis:
        condition: service_healthy
    env_file:
      - .env
    networks:
      - core-network
    restart: unless-stopped

  ors:
    build:
      context: ./ors
    container_name: ors
    volumes:
      - shared_flags:/ors/flags
    ports:
      - 8082:8082
    networks:
      - core-network
    environment:
      - JAVA_OPTS=-Xmx4G -Xms2G
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8082/ors/v2/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 2m30s
    restart: unless-stopped

  otp:
    build: 
      context: ./otp
    container_name: otp
    volumes:
      - shared_flags:/otp/flags
    environment:
      - JAVA_OPTS=-Xmx5G -Xms3G
    networks:
      - core-network
    ports:
      - "8081:8080"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8080/otp/actuators/health || exit 1"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 7m

volumes:
  postgres_data:
    driver: local
  mongo_data:
    driver: local
  redis_data:
    driver: local
  static_volume:
    driver: local
  logs_volume:
    driver: local
  shared_flags:
    driver: local

networks:
  core-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16